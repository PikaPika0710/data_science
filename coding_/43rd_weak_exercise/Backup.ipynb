{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Overviews\n",
    "\n",
    "> Name of dataset: USVideos (Mainly about trending youtube videos in US from 14.11.2017 -> 14.06.2018)\n",
    "\n",
    "### 1.2 Attribute-information\n",
    ">\tvideo_id-Unique video id\n",
    "\n",
    ">\ttrending_date-the date at which video start trending\n",
    "\n",
    ">\ttitle-Title of video\n",
    "\n",
    ">\tchannel_title-video posted by channel\n",
    "\n",
    ">\tcategory_id-there are 32 Category value\n",
    "\n",
    ">\tpublish_time-at what time video is uplaoded\n",
    "\n",
    ">\ttags-tag given to video\n",
    "\n",
    ">\tviews-no of views\n",
    "\n",
    ">\tlikes-no of likes\n",
    "\n",
    ">\tdislikes-no of dislikes\n",
    "\n",
    ">\tcomment_count-no of comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube=pd.read_csv('USvideos.csv')\n",
    "youtube.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Xử lí json để nhập vào thể loại video dựa trên id video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('US_category_id.json') as f:\n",
    "    categories = json.load(f)['items']\n",
    "category_name = {}\n",
    "for category in categories:\n",
    "    category_name[int(category['id'])] = category['snippet']['title']\n",
    "youtube['category_name'] = youtube['category_id'].map(category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thống kê giá trị độc nhất ở mỗi column\n",
    "youtube.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Số lượng video bị tắt bình luận, đánh giá, số video bị xóa hoặc lỗi và số lượng video theo từng thể loại\n",
    "i=1\n",
    "fig=plt.figure(figsize=(15,15))\n",
    "for x in (['comments_disabled','ratings_disabled','video_error_or_removed','category_id']):\n",
    "    count=youtube[x].value_counts()\n",
    "    fig.add_subplot(2,2,i)\n",
    "    sns.barplot(x=count.index, y=count.values, alpha=0.8)\n",
    "    plt.title('{} vs No of video'.format(x))\n",
    "    plt.ylabel('No of video')\n",
    "    plt.xlabel('{}'.format(x))\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No of tags\n",
    "tags=[x.count(\"|\")+1 for x in youtube[\"tags\"]]\n",
    "youtube[\"No_tags\"]=tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in youtube[\"description\"]:\n",
    "    print(len(str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of title\n",
    "title_len=[len(x) for x in youtube[\"title\"]]\n",
    "youtube[\"len_title\"]=title_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publish_time = pd.to_datetime(youtube['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "youtube['publish_time'] = publish_time.dt.time\n",
    "youtube['publish_date'] = publish_time.dt.date\n",
    "\n",
    "#day at which video is publish\n",
    "youtube['publish_weekday']=publish_time.dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio of view/likes  upto 3 decimal\n",
    "youtube[\"Ratio_View_likes\"]=round(youtube[\"views\"]/youtube[\"likes\"],3)\n",
    "#ratio of view/dislikes  upto 3 decimal\n",
    "youtube[\"Ratio_View_dislikes\"]=round(youtube[\"views\"]/youtube[\"dislikes\"],3)\n",
    "#ratio of view/comment_count  upto 3 decimal\n",
    "youtube[\"Ratio_views_comment_count\"]=round(youtube[\"views\"]/youtube[\"comment_count\"],3)\n",
    "#ratio of likes/dislikes  upto 3 decimal\n",
    "youtube[\"Ratio_likes_dislikes\"]=round(youtube[\"likes\"]/youtube[\"dislikes\"],3)\n",
    "#removing the infinite values\n",
    "youtube=youtube.replace([np.inf, -np.inf], np.nan)\n",
    "youtube=youtube.dropna(how='any',axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(youtube[\"Ratio_View_likes\"]))\n",
    "print(max(youtube[\"Ratio_View_dislikes\"]))\n",
    "print(max(youtube[\"Ratio_views_comment_count\"]))\n",
    "print(max(youtube[\"Ratio_likes_dislikes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube['publish_weekday'] = youtube['publish_weekday'].replace({'Monday':1,\n",
    "                                                             'Tuesday':2,\n",
    "                                                             'Wednesday':3,\n",
    "                                                             'Thursday':4,\n",
    "                                                             'Friday':5,\n",
    "                                                             'Saturday':6,\n",
    "                                                             'Sunday':7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=youtube[\"publish_weekday\"].value_counts()\n",
    "print(count)\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.barplot(x=count.index, y=count.values, alpha=0.8)\n",
    "plt.title('No of videos vs weekdays')\n",
    "plt.ylabel('no of videos')\n",
    "plt.xlabel('weekdays')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sự tương quan giữa các dữ liệu views, likes, dislikes, comment_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = youtube\n",
    "corr = data.corr()\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True,\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing non Correlated coloumns\n",
    "youtube.drop(['video_id', 'trending_date', 'title', 'channel_title', 'publish_time', 'tags', 'thumbnail_link', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed', 'description', 'category_name'], inplace = True ,axis = 1)\n",
    "youtube.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.drop(['publish_date'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Không có giá trị NaN nên không cần xử lý dữ liệu trống\n",
    "#### Xử lí dữ liệu ngoại lệ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm vẽ histogram để nhận biết dạng phân bố\n",
    "def histogram(data):\n",
    "    fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(nrows=2, ncols=2)\n",
    "    n_bins=30\n",
    "    ax0.hist(data['views'], n_bins, density=True, histtype='bar')\n",
    "    ax0.set_title('views')\n",
    "    ax1.hist(data['likes'], n_bins, density=True, histtype='bar')\n",
    "    ax1.set_title('likes')\n",
    "    ax2.hist(data['dislikes'], n_bins, histtype='bar')\n",
    "    ax2.set_title('dislikes')\n",
    "    ax3.hist(data['comment_count'], n_bins, histtype='bar')\n",
    "    ax3.set_title('comment_count')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vẽ dữ liệu cột views, likes, dislikes, comment_count và tiến hành xử lý ngoại lệ\n",
    "histogram(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dữ liệu của cột views, likes, dislikes, comment_count có dạng phân bố lệch (skewed) nên ta có: \n",
    "* ==> + Biên trên = 3rd Quantile + 3*IQR\n",
    "* ==> + Biên dưới =  1st Quantile - 3*IQR\n",
    "* IQR: Interquantile range\n",
    "    * 3rd Quantile = Percentile 75\n",
    "    * 1st Quantile = Percentile 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lý ngoại lệ cho cột likes\n",
    "IQR_likes = data['likes'].quantile(0.75) - data['likes'].quantile(0.25)  \n",
    "ub_likes = data['likes'].quantile(0.75) + 3 * IQR_likes \n",
    "lb_likes = data['likes'].quantile(0.25) - 3 * IQR_likes \n",
    "print(ub_likes)\n",
    "print(lb_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lý ngoại lệ cho cột dislikes\n",
    "IQR_dislikes = data['dislikes'].quantile(0.75) - data['dislikes'].quantile(0.25)  \n",
    "ub_dislikes = data['dislikes'].quantile(0.75) + 3 * IQR_dislikes\n",
    "lb_dislikes = data['dislikes'].quantile(0.25) - 3 * IQR_dislikes\n",
    "print(ub_dislikes)\n",
    "print(lb_dislikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lý ngoại lệ cho cột comment_count\n",
    "IQR_comments = data['comment_count'].quantile(0.75) - data['comment_count'].quantile(0.25)  \n",
    "ub_comments = data['comment_count'].quantile(0.75) + 3 * IQR_comments\n",
    "lb_comments = data['comment_count'].quantile(0.25) - 3 * IQR_comments\n",
    "print(ub_comments)\n",
    "print(lb_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lý ngoại lệ cho cột views\n",
    "IQR_views = data['views'].quantile(0.75) - data['views'].quantile(0.25)  \n",
    "ub_views = data['views'].quantile(0.75) + 3 * IQR_views\n",
    "lb_views = data['views'].quantile(0.25) - 3 * IQR_views\n",
    "print(ub_views)\n",
    "print(lb_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mang du lieu da qua xu li ngoai le\n",
    "data_copy = data.copy()\n",
    "# mang du lieu chua qua xu li ngoai le\n",
    "data_copy_non_pr = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xác định các giá trị biên trên và biên dưới của dữ liệu\n",
    "# Thay thế giá trị ngoại lệ bằng 1 trong 2 giá trị trên\n",
    "data_copy.loc[data_copy['views'] >= ub_views, 'views'] = ub_views\n",
    "data_copy.loc[data_copy['likes'] >= ub_likes, 'likes'] = ub_likes\n",
    "data_copy.loc[data_copy['dislikes'] >= ub_dislikes, 'dislikes'] = ub_dislikes\n",
    "data_copy.loc[data_copy['comment_count'] >= ub_comments, 'comment_count'] = ub_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(data_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTING VIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drop_view=data_copy.drop(['views'],axis=1,inplace=False)\n",
    "data_non_pr_drop_view=data_copy_non_pr.drop(['views'],axis=1,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_accuracy = []\n",
    "non_handle_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "for random_state in range(10):\n",
    "    # 1 la da duoc xu li\n",
    "    # 2 la chua duoc xu li\n",
    "    train1,test1,y_train1,y_test1=train_test_split(data_drop_view,data_copy['views'], test_size=0.2,shuffle=False, random_state=random_state)\n",
    "    train2,test2,y_train2,y_test2=train_test_split(data_non_pr_drop_view,data_copy_non_pr['views'], test_size=0.2,shuffle=False, random_state=random_state)\n",
    "    model.fit(train1, y_train1) \n",
    "    y_pred1 = model.predict(test1)\n",
    "\n",
    "    model.fit(train2, y_train2) \n",
    "    y_pred2 = model.predict(test2)\n",
    "    handle_accuracy.append(r2_score(y_test1, y_pred1))\n",
    "    non_handle_accuracy.append(r2_score(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {'True Labels': y_test2, 'Predicted Labels': y_pred2}\n",
    "SK = pd.DataFrame(data = d1)\n",
    "print(SK)\n",
    "lm1 = sns.lmplot(x=\"True Labels\", y=\"Predicted Labels\", data = SK, height = 10)\n",
    "fig1 = lm1.fig \n",
    "fig1.suptitle(\"Sklearn \", fontsize=18)\n",
    "sns.set(font_scale = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", mean(non_handle_accuracy))\n",
    "print(\"Accuracy after handle exception: \", mean(handle_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nhận xét:\n",
    "Việc xử lí ngoại lệ outliers có thể làm giảm hiệu suất của thuật toán LinearRegression (làm mất tính tổng quát)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuan hoa du lieu MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "model = LinearRegression()\n",
    "\n",
    "for random_state in range(10):\n",
    "    # 1 la da duoc xu li\n",
    "    # 2 la chua duoc xu li\n",
    "    train1,test1,y_train1,y_test1=train_test_split(data_drop_view,data_copy['views'], test_size=0.2,shuffle=False, random_state=random_state)\n",
    "    train2,test2,y_train2,y_test2=train_test_split(data_non_pr_drop_view,data_copy_non_pr['views'], test_size=0.2,shuffle=False, random_state=random_state)\n",
    "    # train1 = scaler.fit_transform(train1)\n",
    "    # test1 = scaler.transform(test1)\n",
    "    # train2 = scaler.transform(train2)\n",
    "    # test2 = scaler.transform(test2)\n",
    "\n",
    "    model.fit(train1, y_train1) \n",
    "    y_pred1 = model.predict(test1)\n",
    "\n",
    "    model.fit(train2, y_train2) \n",
    "    y_pred2 = model.predict(test2)\n",
    "    handle_accuracy.append(r2_score(y_test1, y_pred1))\n",
    "    non_handle_accuracy.append(r2_score(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", mean(non_handle_accuracy))\n",
    "print(\"Accuracy after handle exception: \", mean(handle_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raindom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# nEstimator = [140,160,180,200,220]\n",
    "# depth = [10,15,20,25,30]\n",
    "\n",
    "# RF = RandomForestRegressor()\n",
    "# hyperParam = [{'n_estimators':nEstimator,'max_depth': depth}]\n",
    "# gsv = GridSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='r2',n_jobs=-1)\n",
    "# gsv.fit(train2, y_train2)\n",
    "# print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "# print(gsv.best_score_)\n",
    "# scores = gsv.cv_results_['mean_test_score'].reshape(len(nEstimator),len(depth))\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "# plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "# plt.xlabel('n_estimators')\n",
    "# plt.ylabel('max_depth')\n",
    "# plt.colorbar()\n",
    "# plt.xticks(np.arange(len(nEstimator)), nEstimator)\n",
    "# plt.yticks(np.arange(len(depth)), depth)\n",
    "# plt.title('Grid Search r^2 Score')\n",
    "# plt.show()\n",
    "# maxDepth=gsv.best_params_['max_depth']\n",
    "# nEstimators=gsv.best_params_['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# model = RandomForestRegressor(n_estimators = nEstimators,max_depth=maxDepth)\n",
    "# model.fit(train2, y_train2)\n",
    "\n",
    "\n",
    "# # predicting the  test set results\n",
    "# y_pred = model.predict(test2)\n",
    "# print('Root means score', np.sqrt(mean_squared_error(y_test2, y_pred)))\n",
    "# print('Variance score: %.2f' % r2_score(y_test2, y_pred))\n",
    "# print(\"Result :\",model.score(test2, y_test2))\n",
    "# d1 = {'True Labels': y_test2, 'Predicted Labels': y_pred}\n",
    "# SK = pd.DataFrame(data = d1)\n",
    "# print(SK)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4d2420c1c86c857840e9cd37780cab37d65462fab81e90d4a4f57a598e20a77"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
